{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problema\n",
    "\n",
    "El problema consiste en predecir la probabilidad de atraso de los vuelos que aterrizan o despegan del aeropuerto de Santiago\n",
    "de Chile (SCL). Para eso les entregamos un dataset usando datos públicos y reales donde cada fila corresponde a un vuelo\n",
    "que aterrizó o despegó de SCL. Para cada vuelo se cuenta con la siguiente información:\n",
    "\n",
    "* **Fecha-I** : Fecha y hora programada del vuelo.\n",
    "* **Vlo-I** : Número de vuelo programado.\n",
    "* **Ori-I** : Código de ciudad de origen programado.\n",
    "* **Des-I** : Código de ciudad de destino programado.\n",
    "* **Emp-I** : Código aerolínea de vuelo programado.\n",
    "* **Fecha-O** : Fecha y hora de operación del vuelo.\n",
    "* **Vlo-O** : Número de vuelo de operación del vuelo.\n",
    "* **Ori-O** : Código de ciudad de origen de operación\n",
    "* **Des-O** : Código de ciudad de destino de operación.\n",
    "* **Emp-O** : Código aerolínea de vuelo operado.\n",
    "* **DIA** : Día del mes de operación del vuelo.\n",
    "* **MES** : Número de mes de operación del vuelo.\n",
    "* **AÑO** : Año de operación del vuelo.\n",
    "* **DIANOM** : Día de la semana de operación del vuelo.\n",
    "* **TIPOVUELO** : Tipo de vuelo, I =Internacional, N =Nacional.\n",
    "* **OPERA** : Nombre de aerolínea que opera.\n",
    "* **SIGLAORI** : Nombre ciudad origen.\n",
    "* **SIGLADES** : Nombre ciudad destino."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Cargar Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "import xgboost as xgb\n",
    "from xgboost import plot_importance\n",
    "from sklearn.utils import resample\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "import seaborn as sn\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definición de Columnas Basales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNAS_BASALES = [\"OPERA\", \"TIPOVUELO\", \"MES\"]\n",
    "model_dictionary = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creación de Funciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# funcion de upsampling realizada en notebook de Datascientist (implemenatada por completitud)\n",
    "def upsample_data(X, Y):\n",
    "    index_puntual = Y == 0\n",
    "    index_retraso = np.logical_not(index_puntual)\n",
    "    data_no_retraso = X[index_puntual]\n",
    "    data_atraso = X[index_retraso]\n",
    "\n",
    "    data_atraso_upsampled = resample(data_atraso, \n",
    "                                    replace = True,     \n",
    "                                    n_samples = int(0.35*X.shape[0]))\n",
    "    Y1 = resample(Y[index_retraso],\n",
    "                  replace = True,     \n",
    "                    n_samples = int(0.35*X.shape[0]))\n",
    "    \n",
    "    Y0 = Y[index_puntual]\n",
    "    data_upsampled = pd.concat([data_no_retraso, data_atraso_upsampled])\n",
    "    Y = pd.concat([Y0, Y1])\n",
    "    return data_upsampled, Y\n",
    "\n",
    "\n",
    "def prepare_categorical_data(data, columns):\n",
    "\n",
    "    ## VARIABLES:\n",
    "    # data [Dataframe]: El Dataframe donde está la data\n",
    "    # columns: nombre de las columnas que deben ser transformadas a variables dummy\n",
    "\n",
    "    # funcion que se utiliza para preparar los datos para que sean consumidos por el modelo \n",
    "    categorical_transformed_columns = [pd.get_dummies(data[a_column], prefix = a_column) for a_column in columns]\n",
    "    features = pd.concat(categorical_transformed_columns, axis = 1)\n",
    "    #enc = LabelEncoder()\n",
    "    #features = data.loc[:, columns].apply(enc.fit_transform)\n",
    "    return features\n",
    "\n",
    "def generate_splitted_XY(data, columns=COLUMNAS_BASALES, stratify=False): \n",
    "    # funcion que genera los conjuntos de entrenamiento y testeo: \n",
    "    # data [DataFrame]: los datos que tenemos q disposicion.\n",
    "    # columns: las columnas que queremos tranformar a dummy\n",
    "    # stratify: permite hacer los conjuntos de testeo y validación de una manera estratificada. \n",
    "    X, Y = divide_XY(data, columns)\n",
    "    train_test_kwargs = {}\n",
    "    if stratify:\n",
    "        train_test_kwargs.update({ \"stratify\": Y})\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42, **train_test_kwargs)\n",
    "    return x_train, x_test, y_train, y_test, X, Y\n",
    "\n",
    "def divide_XY(data, columns=COLUMNAS_BASALES):\n",
    "    X = prepare_categorical_data(data, columns)\n",
    "    Y = data[\"atraso_15\"]\n",
    "    return X, Y\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- ## 4. Cargar datos -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cargar datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Fecha-I</th>\n",
       "      <th>Vlo-I</th>\n",
       "      <th>Ori-I</th>\n",
       "      <th>Des-I</th>\n",
       "      <th>Emp-I</th>\n",
       "      <th>Fecha-O</th>\n",
       "      <th>Vlo-O</th>\n",
       "      <th>Ori-O</th>\n",
       "      <th>Des-O</th>\n",
       "      <th>Emp-O</th>\n",
       "      <th>...</th>\n",
       "      <th>AÑO</th>\n",
       "      <th>DIANOM</th>\n",
       "      <th>TIPOVUELO</th>\n",
       "      <th>OPERA</th>\n",
       "      <th>SIGLAORI</th>\n",
       "      <th>SIGLADES</th>\n",
       "      <th>temporada_alta</th>\n",
       "      <th>dif_min</th>\n",
       "      <th>atraso_15</th>\n",
       "      <th>periodo_dia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2017-01-01 23:30:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>2017-01-01 23:33:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Domingo</td>\n",
       "      <td>I</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>Miami</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>noche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2017-01-02 23:30:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>2017-01-02 23:39:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Lunes</td>\n",
       "      <td>I</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>Miami</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>noche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-01-03 23:30:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>2017-01-03 23:39:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Martes</td>\n",
       "      <td>I</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>Miami</td>\n",
       "      <td>1</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>noche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-01-04 23:30:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>2017-01-04 23:33:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Miercoles</td>\n",
       "      <td>I</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>Miami</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>noche</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-01-05 23:30:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>2017-01-05 23:28:00</td>\n",
       "      <td>226</td>\n",
       "      <td>SCEL</td>\n",
       "      <td>KMIA</td>\n",
       "      <td>AAL</td>\n",
       "      <td>...</td>\n",
       "      <td>2017</td>\n",
       "      <td>Jueves</td>\n",
       "      <td>I</td>\n",
       "      <td>American Airlines</td>\n",
       "      <td>Santiago</td>\n",
       "      <td>Miami</td>\n",
       "      <td>1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>noche</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Fecha-I Vlo-I Ori-I Des-I Emp-I              Fecha-O Vlo-O  \\\n",
       "0  2017-01-01 23:30:00   226  SCEL  KMIA   AAL  2017-01-01 23:33:00   226   \n",
       "1  2017-01-02 23:30:00   226  SCEL  KMIA   AAL  2017-01-02 23:39:00   226   \n",
       "2  2017-01-03 23:30:00   226  SCEL  KMIA   AAL  2017-01-03 23:39:00   226   \n",
       "3  2017-01-04 23:30:00   226  SCEL  KMIA   AAL  2017-01-04 23:33:00   226   \n",
       "4  2017-01-05 23:30:00   226  SCEL  KMIA   AAL  2017-01-05 23:28:00   226   \n",
       "\n",
       "  Ori-O Des-O Emp-O  ...   AÑO     DIANOM  TIPOVUELO              OPERA  \\\n",
       "0  SCEL  KMIA   AAL  ...  2017    Domingo          I  American Airlines   \n",
       "1  SCEL  KMIA   AAL  ...  2017      Lunes          I  American Airlines   \n",
       "2  SCEL  KMIA   AAL  ...  2017     Martes          I  American Airlines   \n",
       "3  SCEL  KMIA   AAL  ...  2017  Miercoles          I  American Airlines   \n",
       "4  SCEL  KMIA   AAL  ...  2017     Jueves          I  American Airlines   \n",
       "\n",
       "   SIGLAORI SIGLADES temporada_alta dif_min  atraso_15  periodo_dia  \n",
       "0  Santiago    Miami              1     3.0          0        noche  \n",
       "1  Santiago    Miami              1     9.0          0        noche  \n",
       "2  Santiago    Miami              1     9.0          0        noche  \n",
       "3  Santiago    Miami              1     3.0          0        noche  \n",
       "4  Santiago    Miami              1    -2.0          0        noche  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# IMPORTAMOS LOS DATOS DE LA BASE DE DATOS ORIGINAL\n",
    "df_real = pd.read_csv(\"Datasets/dataset_SCL.csv\", low_memory=False)\n",
    "# IMPORTAMOS LAS FEATURES SINTETICAS\n",
    "df_synthetic = pd.read_csv(\"Datasets/synthetic_features.csv\", low_memory=False)\n",
    "# UNIMOS AMBOS DATAFRAMES\n",
    "df = pd.concat([df_real, df_synthetic], axis=1)\n",
    "# LOS MOSTRMOS:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = shuffle(df[['OPERA', 'MES', 'TIPOVUELO', 'SIGLADES', 'SIGLAORI', 'DIANOM', 'atraso_15']], random_state = 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xo, Yo = divide_XY(data)\n",
    "_, xo_test, _, yo_test = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42, stratify=Yo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de vuelos 68206 Cantidad de atrasos 12614 Porcentaje de vuelos atrasados en dataset:0.18493974137172683\n",
      "Cantidad de vuelos train 22508 Cantidad de atrasos 4163 Porcentaje de vuelos atrasados en dataset:0.18495645992535986\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de vuelos {Yo.count()}\",f\"Cantidad de atrasos {Yo.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{Yo.sum()/Yo.count()}\")\n",
    "print(f\"Cantidad de vuelos train {yo_test.count()}\", f\"Cantidad de atrasos {yo_test.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{yo_test.sum()/yo_test.count()}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 Pregunta 1:\n",
    "Escoge el modelo que a tu criterio tenga un mejor performance, argumentando tu decisión.\n",
    "\n",
    "\n",
    "**Respuesta:** \n",
    "La pregunta sobre que modelo es mejor, tiene directa relación con el problema a resolver. Como se indica en el enunciado del Jupyter notebook de Juan, el problema definido es el de predecir el retraso de aviones. De esta manera, a la hora de saber que modelo es mejor es necesario comparar la métrica que mejor represente el comportamiento que queremos del modelo. De este modo, a continuación y a modo de resumen adjunto los resultados obtenidos por los modelos entrenados en el notebook de Juan. \n",
    "\n",
    "### Modelo de Regresión Logística: \n",
    "|   | precision | recall | f1-score | support |\n",
    "|---|-----------|--------|----------|---------|\n",
    "| 0 | 0.82      | 1      | 0.9      | 18403   |\n",
    "| 1 | 0.57      | 0.03   | 0.06     | 4105    |\n",
    "### Modelo XGBoost\n",
    "|   | precision | recall | f1-score | support |\n",
    "|---|-----------|--------|----------|---------|\n",
    "| 0 | 0.82      | 1      | 0.9      | 18403   |\n",
    "| 1 | 0.67      | 0.02   | 0.04     | 4105    |\n",
    "### Modelo XGBoost con mejores Features:\n",
    "|   | precision | recall | f1-score | support |\n",
    "|---|-----------|--------|----------|---------|\n",
    "| 0 | 0.82      | 1      | 0.9      | 18403   |\n",
    "| 1 | 0.66      | 0.01   | 0.03     | 4105    |\n",
    "### Modelo XGBoost con upsampled data \n",
    "|   | precision | recall | f1-score | support |\n",
    "|---|-----------|--------|----------|---------|\n",
    "| 0 | 0.68      | 0.93   | 0.79     | 18403   |\n",
    "| 1 | 0.62      | 0.19   | 0.3      | 9897    |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La predicción de retrasos en aviones para las aerolines tiene beneficios no solamente para los pasajeros, sino que para la aerolínea también. Desde un punto de vista puramente de costos operacionales según <a href=https://www.airlines.org/dataset/u-s-passenger-carrier-delay-costs>Airlines for America</a> el costo por minuto de \"Block time\" ronda los $80 dólares (cifras 2021), este tiempo considera el minuto desde que el avión cierra sus puertas hasta el instante en que este llega a la compuerta en su destino. Considerando lo anterior un avión que sufra retrasos tendrá un costo tanto por su tiempo de vuelo como además el tiempo que utilice para llegar a la compuerta. \n",
    "\n",
    "Además del costo anterior, existe un costo económico que debe ser considerado, ya que según <a href=https://www.sernac.cl/portal/607/w3-article-5594.html>Sernac</a> los pasajeros que sufran por retrasos en sus vuelos tienen, en algunos casos, derecho a una indemnización. Esto sumado a la entrega de una mala experiencia para los usuarios que es fundamental para cualquier aerolínes, ya que parte importante de la competencia existente entre ellas es tanto el precio como la experiencia de viaje que puedan proporcionarle al usuario. \n",
    "\n",
    "Debido a esto, al momento de desarrollar un modelo que realice predicciones sobre el retraso de los vuelos de aviones, la consideración más importante es la cantidad de vuelos que efectivamente llegaron tarde y fueron seleccionados por el modelo. Tener seguridad de que el modelo presentado tiene una alta taza de recall sobre la clase de retraso es importante ya que permite que la aerolínea al saber sobre dicho retraso pueda tomar acción desde el punto de vista de la gestión de tanto el vuelo como del pasajero, además es importante que la tasa de precisión no sea baja ya que también es importante que la detección de atrasos no tenga falsos positivos.  \n",
    "\n",
    "De este modo, si se consideran los modelos entrenados en el notebook de Juan, se podrá ver que el modelo de recall con upsampling tiene el mayor recall de todos, pero al momento de hacer dicho aumento de muestras dicho aumento fue realizado también sobre la clase de prueba, por lo que los resultados encontrados en esa corrida del algoritmo están sesgados a que la distribución de clases presentes en los datos de testeo no son los originales. Debido a esto es necesario correr nuevamente dicho algoritmo."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Muestra original de los  datos"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Modelo Basal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>\n",
    "  <p>Para este modelo las columnas que se utilizarán serán las mismas utilizadas en el notebook anterior. Dichas columnas son las siguientes: </p>\n",
    "  <li>OPERA</li>\n",
    "  <li>TIPOVUELO</li>\n",
    "  <li>MES</li>\n",
    "</ol>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un punto importante a considerar en el notebook, es que al momento de extraer los datos se utilizan las siguientes columnas:\n",
    "'OPERA', 'MES', 'TIPOVUELO', 'SIGLADES', 'DIANOM', 'atraso_15'\n",
    "\n",
    "Pero al momento de generar los datos para entrenar los modelos no son utilizadas todas las columnas. Por esto, el modelo que se genera solamente trabajará con: 'OPERA', 'MES', 'TIPOVUELO'. Lo anterior se hace para poder entrenar el modelo basal. \n",
    "\n",
    "A continuación, se muestra la linea de la celda textual (sección 4.) que genera los datos para los modelos de predicción:\n",
    "\n",
    "features = pd.concat([pd.get_dummies(data['OPERA'], prefix = 'OPERA'),pd.get_dummies(data['TIPOVUELO'], prefix = 'TIPOVUELO'), pd.get_dummies(data['MES'], prefix = 'MES')], axis = 1)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val, X, Y = generate_splitted_XY(data, stratify=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de vuelos 68206 Cantidad de atrasos 12614 Porcentaje de vuelos atrasados en dataset:0.18493974137172683\n",
      "Cantidad de vuelos train 45698 Cantidad de atrasos 8509 Porcentaje de vuelos atrasados en dataset:0.18620070900258218\n",
      "Cantidad de vuelos train 22508 Cantidad de atrasos 4105 Porcentaje de vuelos atrasados en dataset:0.18237959836502576\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de vuelos {Y.count()}\",f\"Cantidad de atrasos {Y.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{Y.sum()/Y.count()}\")\n",
    "print(f\"Cantidad de vuelos train {y_train.count()}\", f\"Cantidad de atrasos {y_train.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_train.sum()/y_train.count()}\")\n",
    "print(f\"Cantidad de vuelos train {y_val.count()}\", f\"Cantidad de atrasos {y_val.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_val.sum()/y_val.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = upsample_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de vuelos 68206 Cantidad de atrasos 12614 Porcentaje de vuelos atrasados en dataset:0.18493974137172683\n",
      "Cantidad de vuelos train 53183 Cantidad de atrasos 15994 Porcentaje de vuelos atrasados en dataset:0.30073519733749504\n",
      "Cantidad de vuelos train 22508 Cantidad de atrasos 4105 Porcentaje de vuelos atrasados en dataset:0.18237959836502576\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de vuelos {Y.count()}\",f\"Cantidad de atrasos {Y.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{Y.sum()/Y.count()}\")\n",
    "print(f\"Cantidad de vuelos train {y_train.count()}\", f\"Cantidad de atrasos {y_train.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_train.sum()/y_train.count()}\")\n",
    "print(f\"Cantidad de vuelos train {y_val.count()}\", f\"Cantidad de atrasos {y_val.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_val.sum()/y_val.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelxgb_base = xgb.XGBClassifier(random_state=1,  objective= 'binary:logistic', learning_rate=0.01, subsample = 1, max_depth = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " no atrasado       0.83      0.98      0.90     18403\n",
      "    atrasado       0.51      0.07      0.13      4105\n",
      "\n",
      "    accuracy                           0.82     22508\n",
      "   macro avg       0.67      0.53      0.51     22508\n",
      "weighted avg       0.77      0.82      0.76     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelxgb_base = modelxgb_base.fit(x_train, y_train,verbose=False)\n",
    "predicted_xgb = modelxgb_base.predict(x_val)\n",
    "print(classification_report(y_val, predicted_xgb, target_names=[\"no atrasado\", \"atrasado\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      " no atrasado       0.82      0.98      0.90     18345\n",
      "    atrasado       0.51      0.07      0.13      4163\n",
      "\n",
      "    accuracy                           0.82     22508\n",
      "   macro avg       0.67      0.53      0.51     22508\n",
      "weighted avg       0.77      0.82      0.75     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = modelxgb_base.predict(xo_test)\n",
    "print(classification_report(yo_test, predicted_xgb, target_names=[\"no atrasado\", \"atrasado\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary[\"modelxgb_base\"]= average_precision_score(yo_test, predicted_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelxgb_base': 0.20839491452833558}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar efectivamente los resultados del algoritmo con oversampling cambiaron bastante cuando se considera la distribución original de los datos. A pesar de eso, el modelo generado presenta mejores cifras que los modelos que los otros modelos generados por ende, el modelo seleccionado es el modelo XGBoost con oversampling. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1.2Pregunta 2: \n",
    "Implementa mejoras sobre el modelo escogiendo la o las técnicas que prefieras\n",
    "\n",
    "### Respuesta: \n",
    "Para mejorar el modelo es necesario considerar en primera instancia que clase de modelo estamos generando y si dicho modelo es el correcto para resolver el problema, además de estos es necesario considerar las funciones objetivo que dicho modelo utiliza para asegurarnos que estas funciones objetivo generan el efecto que se está buscando en este proyecto.\n",
    "De esta manera, se puede descartar que el error de dicha implementación sea el uso de XGBoost, esto se debe a que este modelo es de los mejores modelos utilizados en la página kaggle, en específico en una gran cantidad de problemas de clasificación tanto multiclase como binario. De esta manera, aunque el tipo de modelo que se está utilizando no debe ser el problema talvés la manera en que se está inicializando y evaluando dicho modelo puede tener cierto espacio de mejora. \n",
    "\n",
    "Una de las primeras mejoras propuestas es el uso de un conjunto de validación que le permita al algoritmo ajustarse adecuadamente a la distribución real de los datos. Dicho conjunto de validación no se verá afectado por el efecto del cambio de distribución de los datos, permitiendo de este modo que el modelo sea fidedigno con la distribución original de los datos. Además, se define que la métrica a utilizar es el AUCPR (Área bajo la curva de Precisión-recall) los modelos que se evalúan con las curvas ROC sobre conjuntos de datos desbalanceados, tienden a sufrir de sobre ajusta con la clase mayoritaria, ya que las curvas roc contemplan en su cálculo la tasa de Verdaderos Negativos, que en caso de conjuntos de datos desbalanceados puede ser muy alta. Las curvas Precisión-recall por el otro lado no tienen esta dependencia y por ende no tienen dicho problema. \n",
    "\n",
    "Otra mejora que se implementará será cambiar el tipo de codificación de los datos de one hot encoding a un encoding ordinario, la razón de dicho cambio se debe al algoritmo que se está utilizando, como los algoritmos con árboles de decisión hacen particiones sobre los features, es más fácil para dicho algoritmo separar menos features en comparación a más. \n",
    "\n",
    "De esta manera, para generar mejoras sobre el modelo, en primera instancia se obtendrán los datos generados en el jupyter notebook de Juan y se correrá el modelo con dichos datos, de esta manera, los pasos con los que se intentará mejorar dicho modelo son los siguientes: \n",
    "<ol>\n",
    "  <p>Con los features obtenidos por Juan:</p>\n",
    "  <li>Se correrá el modelo basal con los datos entregados para tener un punto de comparación</li>\n",
    "  <li>Se hará un grid search sobre nuevos parámetros que se le agreguen al modelo para mejorar su desempeño</li>\n",
    "  <li>Se realizará oversampling utilizando métodos más complejos de oversampling como es SMOTE</li>\n",
    "  <li>Se realizará oversampling con SMOTE y random undersampling (siendo este la manera recomendada de corre dicho algoritmo)</li>\n",
    "  <li>Grid Search sobre los parámetros del modelo</li>\n",
    "\n",
    "  <p>Uso de otros features:</p>\n",
    "  <li>Se Utilizarán más features para así enriquecer el modelo generado</li>\n",
    "  <li>Se obtendrán resultados con los mismos métodos de over y under sampling anteriormente mencionados</li>\n",
    "  <li>Grid Search sobre los parámetros del modelo</li>\n",
    "</ol>\n",
    "\n",
    "En cada una de las dos etapas propuestas se generará un conjunto de test el cual viene directamente de los datos muestreados y no presenta modificaciones con respecto a las proporciones originales que presenta dicho dataset. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Grid Search con parámetros de regularización:\n",
    "A modo de conjetura, se cree que el modelo al tener una  profundidad máxima de 10 puede estar sufriendo de sobre ajuste. Además, el modelo de XGBoost utiliza una regularización $\\alpha$ para regularizar mediante la norma $L_{2}$, dicho modelo también tiene otras formas de evitar el sobre ajuste y una posible mejora para el modelo puede ser ajustar dicho parámetro, ya que permite que el árbol sufra en menor medida de sobre ajuste debido a la penalización.   \n",
    "\n",
    "Además, para encontrar el mejor modelo posible lo que haremos será agregar un conjunto de validación el cual se utilizará para hacer la selección del modelo. Como se dijo anteriormente el resultado sobre los datos originales sin cambios en sus procentajes definirá cuál será el modelo que utilizaremos. \n",
    "\n",
    "De este modo, lo que se propone es variar los hiperparámetros que generen mejoras en el sobre ajuste del modelo. En específico, estos parámetros son:\n",
    "  - $\\alpha$\n",
    "  - $\\lambda$\n",
    "  - Profundidad máxima\n",
    "  - scale_pos_weight\n",
    "  - subsample\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generamos la nueva codificacion de features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los datos del dataframe:\n",
    "data_to_pipe = data[COLUMNAS_BASALES]\n",
    "\n",
    "# volvemos a generar los datos originales con la nueva codificacion:\n",
    "Xo, Yo = data_to_pipe, data[\"atraso_15\"]\n",
    "_, xo_test, _, yo_test = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42, stratify=Yo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "# generamos una codificaciones ordinaria para los datos: \n",
    "column_trans = make_column_transformer((OrdinalEncoder(), COLUMNAS_BASALES))\n",
    "# ajustamos la codificacion\n",
    "column_trans.fit(data_to_pipe)\n",
    "\n",
    "# generamos una funcion que genere pipelines: \n",
    "Makepipe = lambda the_model: make_pipeline(column_trans, the_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos un kfold stratificado para la búsqueda de los modelos: \n",
    "skf = StratifiedKFold(n_splits=2, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(Xo, Yo, test_size=0.33, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = upsample_data(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de vuelos train 53183 Cantidad de atrasos 15994 Porcentaje de vuelos atrasados en dataset:0.30073519733749504\n",
      "Cantidad de vuelos validacion 22508 Cantidad de atrasos 4105 Porcentaje de vuelos atrasados en dataset:0.18237959836502576\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de vuelos train {y_train.count()}\", f\"Cantidad de atrasos {y_train.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_train.sum()/y_train.count()}\")\n",
    "print(f\"Cantidad de vuelos validacion {y_val.count()}\", f\"Cantidad de atrasos {y_val.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_val.sum()/y_val.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# para poder hacer correctamente el grid search se hace la codificacion de las entradas\n",
    "x_train, x_val, xo_test= column_trans.transform(x_train), column_trans.transform(x_val), column_trans.transform(xo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelxgb_grid = xgb.XGBClassifier()\n",
    "parameters = {\"eta\": [0.01],\n",
    "    \"scale_pos_weight\": [1, 1.2, 3, 5],\n",
    "    \"reg_lambda\": [0, 0.5, 1],\n",
    "    \"reg_alpha\": [0, 0.5, 1],\n",
    "    \"gamma\": [0, 0.25, 1],\n",
    "    \"subsample\": [1],\n",
    "    \"max_depth\": [10]}\n",
    "xgb_grid_base = GridSearchCV(modelxgb_grid, param_grid=parameters,\n",
    "    cv = 5, n_jobs=-1, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;eta&#x27;: [0.01], &#x27;gamma&#x27;: [0, 0.25, 1],\n",
       "                         &#x27;max_depth&#x27;: [10], &#x27;reg_alpha&#x27;: [0, 0.5, 1],\n",
       "                         &#x27;reg_lambda&#x27;: [0, 0.5, 1],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 1.2, 3, 5], &#x27;subsample&#x27;: [1]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;eta&#x27;: [0.01], &#x27;gamma&#x27;: [0, 0.25, 1],\n",
       "                         &#x27;max_depth&#x27;: [10], &#x27;reg_alpha&#x27;: [0, 0.5, 1],\n",
       "                         &#x27;reg_lambda&#x27;: [0, 0.5, 1],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 1.2, 3, 5], &#x27;subsample&#x27;: [1]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'eta': [0.01], 'gamma': [0, 0.25, 1],\n",
       "                         'max_depth': [10], 'reg_alpha': [0, 0.5, 1],\n",
       "                         'reg_lambda': [0, 0.5, 1],\n",
       "                         'scale_pos_weight': [1, 1.2, 3, 5], 'subsample': [1]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid_base.fit(x_train, y_train, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.90     18403\n",
      "           1       0.48      0.11      0.17      4105\n",
      "\n",
      "    accuracy                           0.82     22508\n",
      "   macro avg       0.66      0.54      0.54     22508\n",
      "weighted avg       0.77      0.82      0.76     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = xgb_grid_base.predict(x_val)\n",
    "print(classification_report(y_val, predicted_xgb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.97      0.90     18345\n",
      "           1       0.49      0.11      0.18      4163\n",
      "\n",
      "    accuracy                           0.81     22508\n",
      "   macro avg       0.66      0.54      0.54     22508\n",
      "weighted avg       0.76      0.81      0.76     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = xgb_grid_base.predict(xo_test)\n",
    "print(classification_report(yo_test, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary[\"xgb_grid_base\"]= average_precision_score(yo_test, predicted_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta': 0.01,\n",
       " 'gamma': 1,\n",
       " 'max_depth': 10,\n",
       " 'reg_alpha': 1,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid_base.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ronda 2: \n",
    "Los parámetros que quedaron en posiciones extremas y que dichas posiciones no sean el final de su rango, serán re ajustadas nuevamente para ver si es posible encontrar un nuevo óptimo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelxgb_grid_round2 = xgb.XGBClassifier(random_state=1, objective= 'binary:logistic', subsample=0.9, eval_metric='aucpr')\n",
    "parameters = {\"eta\": [0.01],\n",
    "    \"scale_pos_weight\": [1],\n",
    "    \"reg_lambda\": [1, 1.5, 3],\n",
    "    \"reg_alpha\": [1, 1.5, 3],\n",
    "    \"gamma\": [1],\n",
    "    \"subsample\": [1],\n",
    "    \"max_depth\": [10]}\n",
    "clf_grid_round2 = GridSearchCV(modelxgb_grid_round2, param_grid=parameters,\n",
    "    cv = skf, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 9 candidates, totalling 18 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=2, random_state=42, shuffle=True),\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;aucpr&#x27;, feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=1, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;eta&#x27;: [0.01], &#x27;gamma&#x27;: [1], &#x27;max_depth&#x27;: [10],\n",
       "                         &#x27;reg_alpha&#x27;: [1, 1.5, 3], &#x27;reg_lambda&#x27;: [1, 1.5, 3],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1], &#x27;subsample&#x27;: [1]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=2, random_state=42, shuffle=True),\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric=&#x27;aucpr&#x27;, feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=1, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;eta&#x27;: [0.01], &#x27;gamma&#x27;: [1], &#x27;max_depth&#x27;: [10],\n",
       "                         &#x27;reg_alpha&#x27;: [1, 1.5, 3], &#x27;reg_lambda&#x27;: [1, 1.5, 3],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1], &#x27;subsample&#x27;: [1]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;aucpr&#x27;, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=1, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;aucpr&#x27;, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=1, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=2, random_state=42, shuffle=True),\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False,\n",
       "                                     eval_metric='aucpr', feature_types=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_...\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=1, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'eta': [0.01], 'gamma': [1], 'max_depth': [10],\n",
       "                         'reg_alpha': [1, 1.5, 3], 'reg_lambda': [1, 1.5, 3],\n",
       "                         'scale_pos_weight': [1], 'subsample': [1]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_grid_round2.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     18403\n",
      "           1       0.42      0.13      0.20      4105\n",
      "\n",
      "    accuracy                           0.81     22508\n",
      "   macro avg       0.63      0.55      0.55     22508\n",
      "weighted avg       0.76      0.81      0.77     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = clf_grid_round2.predict(x_val)\n",
    "print(classification_report(y_val, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.96      0.89     18345\n",
      "           1       0.43      0.14      0.21      4163\n",
      "\n",
      "    accuracy                           0.81     22508\n",
      "   macro avg       0.63      0.55      0.55     22508\n",
      "weighted avg       0.76      0.81      0.76     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = clf_grid_round2.predict(xo_test)\n",
    "print(classification_report(yo_test, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary[\"xgb_grid_base_round2\"]= average_precision_score(yo_test, predicted_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelxgb_base': 0.20839491452833558,\n",
       " 'xgb_grid_base': 0.21728632479901608,\n",
       " 'xgb_grid_base_round2': 0.2185626724133346}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_best = xgb_grid_base.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta': 0.01,\n",
       " 'gamma': 1,\n",
       " 'max_depth': 10,\n",
       " 'reg_alpha': 1,\n",
       " 'reg_lambda': 1,\n",
       " 'scale_pos_weight': 1,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_best"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar con estos resultados, el uso de nueva codificación y mayor regularización generaron que el modelo mejorar sus métricas de área bajo la curva PR. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Oversampling SMOTE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=https://arxiv.org/pdf/1106.1813.pdf>SMOTE</a> (Synthetic Minority Over-Sampling Technique) es un paper publicado el año 2002 en la revista Journal of Artificial Intelligence Research, el método propuesto se plantea como una opción para el método de undersampling o oversampling que se utiliza en muchos casos en la práctica cuando se tienen clases desbalanceadas. En específico, lo que hace este método es generar datos sintéticos de la clase minoritaria para así generar un oversampling de esta, en específico, el método genera muestras utilizando los k-vecinos más cercanos y realizando una perturbación de los features de la muestra en la dirección de dicho vecino. \n",
    "\n",
    "El método al hacer esta generación de datos sintéticos en la dirección de uno de sus vecinos más cercanos lo que hace es forzar la generalización de la zona de decisión de la clase minoritaria. Como hiper parpametro este algoritmo utiliza el porcentaje de oversampling que se desea realizar de esta. Además, en este trabajo utilizan una mezcla entre primero hacer uso del algoritmo SMOTE y luego realizar un undersampling sobre la clase mayoritaria. Para intentar mejorar dicho modelo haremos los dos métodos y veremos cual es mejor. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 SMOTE OverSample:\n",
    "Para este caso se partirá realizando solamente un oversampling de la clase minoritaria, en específico, se buscará que dicha clase represente aproximadamente el 70% de los datos de la clase mayoritaria. Actualmente los datos de la clase **retraso** representan aproximadamente el 18% de la base de datos. Esto implica que realizaremos un aumento de 166% de los datos, esto con el fin de verificar si mediante el método SMOTE efectivamente generan mejoras considerables en los resultados encontrados en comparación al método basal.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "x_train, x_val, y_train, y_val = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42)\n",
    "x_train, x_val = column_trans.transform(x_train), column_trans.transform(x_val)\n",
    "oversample = SMOTE(sampling_strategy=0.7, random_state=42, k_neighbors=10)\n",
    "x_train, y_train = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de vuelos 68206 Cantidad de atrasos 12614 Porcentaje de vuelos atrasados en dataset:0.18493974137172683\n",
      "Cantidad de vuelos train 63221 Cantidad de atrasos 26032 Porcentaje de vuelos atrasados en dataset:0.4117619145537084\n",
      "Cantidad de vuelos validation 22508 Cantidad de atrasos 4105 Porcentaje de vuelos atrasados en dataset:0.18237959836502576\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de vuelos {Y.count()}\",f\"Cantidad de atrasos {Y.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{Y.sum()/Y.count()}\")\n",
    "print(f\"Cantidad de vuelos train {y_train.count()}\", f\"Cantidad de atrasos {y_train.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_train.sum()/y_train.count()}\")\n",
    "print(f\"Cantidad de vuelos validation {y_val.count()}\", f\"Cantidad de atrasos {y_val.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_val.sum()/y_val.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84     18403\n",
      "           1       0.31      0.37      0.34      4105\n",
      "\n",
      "    accuracy                           0.74     22508\n",
      "   macro avg       0.58      0.60      0.59     22508\n",
      "weighted avg       0.76      0.74      0.75     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelxgb_smote = xgb.XGBClassifier(random_state=1,  objective= 'binary:logistic',  eval_metric='aucpr',**previous_best)\n",
    "modelxgb_smote = modelxgb_smote.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)\n",
    "predicted_xgb = modelxgb_smote.predict(x_val)\n",
    "print(classification_report(y_val, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.82      0.84     18345\n",
      "           1       0.33      0.39      0.36      4163\n",
      "\n",
      "    accuracy                           0.74     22508\n",
      "   macro avg       0.59      0.60      0.60     22508\n",
      "weighted avg       0.76      0.74      0.75     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = modelxgb_smote.predict(xo_test)\n",
    "print(classification_report(yo_test, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary[\"xgb_smote\"]= average_precision_score(yo_test, predicted_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelxgb_base': 0.20839491452833558,\n",
       " 'xgb_grid_base': 0.21728632479901608,\n",
       " 'xgb_grid_base_round2': 0.2185626724133346,\n",
       " 'xgb_smote': 0.24054706513149243}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez utilizado el algoritmo SMOTE se puede observar una mejora del 20% en los resultados obtenidos."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 SMOTE y Random UnderSampling:\n",
    "\n",
    "Otra manera de utilizar el método SMOTE es mediante el oversampling de la clase minoritaria para luego realizar un under sampling de la clase mayoritaria. Esto permite compensar un poco la pérdida de información por el under sampling mediante la generación de muestras sintéticas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, xo_test, _, yo_test = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42, stratify=Yo)\n",
    "x_train, x_val, y_train, y_val = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42)\n",
    "x_train, x_val, xo_test = column_trans.transform(x_train), column_trans.transform(x_val), column_trans.transform(xo_test)\n",
    "oversample = SMOTE(sampling_strategy=0.5, random_state=1)\n",
    "x_train, x_val, y_train, y_val = train_test_split(X, Y, test_size = 0.33, random_state = 42)\n",
    "x_train, y_train = oversample.fit_resample(x_train, y_train)\n",
    "under = RandomUnderSampler(sampling_strategy=0.7, random_state=1)\n",
    "x_train, y_train = under.fit_resample(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cantidad de vuelos 68206 Cantidad de atrasos 12614 Porcentaje de vuelos atrasados en dataset:0.18493974137172683\n",
      "Cantidad de vuelos train 45156 Cantidad de atrasos 18594 Porcentaje de vuelos atrasados en dataset:0.4117725219239968\n",
      "Cantidad de vuelos train 22508 Cantidad de atrasos 4105 Porcentaje de vuelos atrasados en dataset:0.18237959836502576\n"
     ]
    }
   ],
   "source": [
    "print(f\"Cantidad de vuelos {Y.count()}\",f\"Cantidad de atrasos {Y.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{Y.sum()/Y.count()}\")\n",
    "print(f\"Cantidad de vuelos train {y_train.count()}\", f\"Cantidad de atrasos {y_train.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_train.sum()/y_train.count()}\")\n",
    "print(f\"Cantidad de vuelos train {y_val.count()}\", f\"Cantidad de atrasos {y_val.sum()}\", f\"Porcentaje de vuelos atrasados en dataset:{y_val.sum()/y_val.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83     18403\n",
      "           1       0.31      0.38      0.35      4105\n",
      "\n",
      "    accuracy                           0.73     22508\n",
      "   macro avg       0.58      0.60      0.59     22508\n",
      "weighted avg       0.76      0.73      0.74     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelxgb_smote_under = xgb.XGBClassifier(random_state=1,  objective= 'binary:logistic', eval_metric='aucpr',  **previous_best)\n",
    "modelxgb_smote_under = modelxgb_smote_under.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)\n",
    "predicted_xgb = modelxgb_smote_under.predict(x_val)\n",
    "print(classification_report(y_val, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.82      0.84     18345\n",
      "           1       0.33      0.40      0.36      4163\n",
      "\n",
      "    accuracy                           0.74     22508\n",
      "   macro avg       0.59      0.61      0.60     22508\n",
      "weighted avg       0.76      0.74      0.75     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = modelxgb_smote_under.predict(xo_test)\n",
    "print(classification_report(yo_test, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary[\"xgb_smote_under\"]= average_precision_score(yo_test, predicted_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelxgb_base': 0.20839491452833558,\n",
       " 'xgb_grid_base': 0.21728632479901608,\n",
       " 'xgb_grid_base_round2': 0.2185626724133346,\n",
       " 'xgb_smote': 0.24054706513149243,\n",
       " 'xgb_smote_under': 0.24140623473961104,\n",
       " 'xgb_grid_smote': 0.2415843165271429,\n",
       " 'more_features': 0.2376063877394071,\n",
       " 'xgb_important_features': 0.23492118067480844}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Selección Mejor par Modelo-Método de Over-Under Sampling:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se indicó anteriormente, la manera en que se definirá el mejor modelo para el set de datos generado es mediante la métrica de AUCPR, dicha métrica nos permite encontrar el mejor modelo que considere tanto el recall como la precisión. De esta manera, el modelo elegido es el uso de SMOTE sin under sampling."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Grid Search Mejor Modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42)\n",
    "x_train, x_val = column_trans.transform(x_train), column_trans.transform(x_val)\n",
    "oversample = SMOTE(sampling_strategy=0.7, random_state=42, k_neighbors=10)\n",
    "x_train, y_train = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "modelxgb_smote_grid = xgb.XGBClassifier(random_state=1,  objective= 'binary:logistic',  eval_metric='aucpr', **previous_best)\n",
    "parameters = { \"reg_lambda\": [0, 0.1, 1],\n",
    "                \"scale_pos_weight\": [1, 20, 30,  1000],\n",
    "                \"subsample\": [0.5, 1],\n",
    "                \"eta\": [0.01, 0.1, 1],\n",
    "                \"gamma\": [0, 0.25, 1]}\n",
    "xgb_smote_grid = GridSearchCV(modelxgb_grid, param_grid=parameters,\n",
    "    cv =2, n_jobs=-1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 216 candidates, totalling 432 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;eta&#x27;: [0.01, 0.1, 1], &#x27;gamma&#x27;: [0, 0.25, 1],\n",
       "                         &#x27;reg_lambda&#x27;: [0, 0.1, 1],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 20, 30, 1000],\n",
       "                         &#x27;subsample&#x27;: [0.5, 1]},\n",
       "             verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=2,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;eta&#x27;: [0.01, 0.1, 1], &#x27;gamma&#x27;: [0, 0.25, 1],\n",
       "                         &#x27;reg_lambda&#x27;: [0, 0.1, 1],\n",
       "                         &#x27;scale_pos_weight&#x27;: [1, 20, 30, 1000],\n",
       "                         &#x27;subsample&#x27;: [0.5, 1]},\n",
       "             verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=2,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     feature_types=None, gamma=None,\n",
       "                                     gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None,...\n",
       "                                     max_cat_to_onehot=None,\n",
       "                                     max_delta_step=None, max_depth=None,\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, ...),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'eta': [0.01, 0.1, 1], 'gamma': [0, 0.25, 1],\n",
       "                         'reg_lambda': [0, 0.1, 1],\n",
       "                         'scale_pos_weight': [1, 20, 30, 1000],\n",
       "                         'subsample': [0.5, 1]},\n",
       "             verbose=1)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_smote_grid.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83     18403\n",
      "           1       0.31      0.39      0.35      4105\n",
      "\n",
      "    accuracy                           0.73     22508\n",
      "   macro avg       0.59      0.60      0.59     22508\n",
      "weighted avg       0.76      0.73      0.74     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = xgb_smote_grid.predict(x_val)\n",
    "print(classification_report(y_val, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.83     18345\n",
      "           1       0.33      0.40      0.36      4163\n",
      "\n",
      "    accuracy                           0.74     22508\n",
      "   macro avg       0.59      0.61      0.60     22508\n",
      "weighted avg       0.76      0.74      0.75     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = xgb_smote_grid.predict(xo_test)\n",
    "print(classification_report(yo_test, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eta': 1, 'gamma': 0, 'reg_lambda': 0, 'scale_pos_weight': 1, 'subsample': 1}"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_smote_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary[\"xgb_grid_smote\"]= average_precision_score(yo_test, predicted_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelxgb_base': 0.20839491452833558,\n",
       " 'xgb_grid_base': 0.21728632479901608,\n",
       " 'xgb_grid_base_round2': 0.2185626724133346,\n",
       " 'xgb_smote': 0.24054706513149243,\n",
       " 'xgb_smote_under': 0.2284234527919285,\n",
       " 'xgb_grid_smote': 0.2415843165271429}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_best = xgb_smote_grid.best_params_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De este modo, viendo el diccionario de valores, se puede ver que el mejor valor obtenido es el con xgb_grid_smote"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Entrenando modelo agregando features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLUMNAS_MAS_FEATURES = ['OPERA', 'MES', 'TIPOVUELO', 'SIGLADES', \"SIGLAORI\",'DIANOM']\n",
    "data_to_pipe = data[COLUMNAS_MAS_FEATURES]\n",
    "Xo, Yo = data_to_pipe, data[\"atraso_15\"]\n",
    "_, xo_test, _, yo_test = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42, stratify=Yo)\n",
    "# generamos una codificaciones ordinaria para los datos: \n",
    "column_trans = make_column_transformer((OrdinalEncoder(), COLUMNAS_MAS_FEATURES))\n",
    "# ajustamos la codificacion\n",
    "column_trans.fit(data_to_pipe)\n",
    "xo_test = column_trans.transform(xo_test)\n",
    "\n",
    "# generamos una funcion que genere pipelines: \n",
    "Makepipe = lambda the_model: make_pipeline(column_trans, the_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42)\n",
    "x_train, x_val = column_trans.transform(x_train), column_trans.transform(x_val)\n",
    "oversample = SMOTE(sampling_strategy=0.7, random_state=42, k_neighbors=10)\n",
    "x_train, y_train = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.88     18403\n",
      "           1       0.40      0.18      0.25      4105\n",
      "\n",
      "    accuracy                           0.80     22508\n",
      "   macro avg       0.62      0.56      0.57     22508\n",
      "weighted avg       0.76      0.80      0.77     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelxgb = xgb.XGBClassifier(random_state=1, objective= 'binary:logistic',  eval_metric='aucpr', **previous_best)\n",
    "modelxgb = modelxgb.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)\n",
    "predicted_xgb = modelxgb.predict(x_val)\n",
    "print(classification_report(y_val, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.94      0.89     18345\n",
      "           1       0.44      0.21      0.28      4163\n",
      "\n",
      "    accuracy                           0.80     22508\n",
      "   macro avg       0.64      0.57      0.58     22508\n",
      "weighted avg       0.77      0.80      0.77     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predicted_xgb = modelxgb.predict(xo_test)\n",
    "print(classification_report(yo_test, predicted_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary[\"more_features\"] = average_precision_score(yo_test, predicted_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelxgb_base': 0.20839491452833558,\n",
       " 'xgb_grid_base': 0.21728632479901608,\n",
       " 'xgb_grid_base_round2': 0.2185626724133346,\n",
       " 'xgb_smote': 0.24054706513149243,\n",
       " 'xgb_smote_under': 0.2284234527919285,\n",
       " 'xgb_grid_smote': 0.2415843165271429,\n",
       " 'more_features': 0.2376063877394071}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dictionary"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Entrenando modelo solo con features importantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_important = xgb_smote_grid.best_estimator_.get_booster().get_score(importance_type='weight')\n",
    "keys = list(feature_important.keys())\n",
    "values = list(feature_important.values())\n",
    "\n",
    "important_features = pd.DataFrame(data=values, index=keys, columns=[\"score\"]).sort_values(by = \"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = Xo.iloc[:, [0, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>f2</th>\n",
       "      <td>1707.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f0</th>\n",
       "      <td>1399.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>117.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     score\n",
       "f2  1707.0\n",
       "f0  1399.0\n",
       "f1   117.0"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "important_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los datos del dataframe:\n",
    "data_to_pipe = data[[\"OPERA\", \"MES\"]]\n",
    "\n",
    "# volvemos a generar los datos originales con la nueva codificacion:\n",
    "Xo, Yo = data_to_pipe, data[\"atraso_15\"]\n",
    "_, xo_test, _, yo_test = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42, stratify=Yo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una codificaciones ordinaria para los datos: \n",
    "column_trans = make_column_transformer((OrdinalEncoder(), [\"OPERA\", \"MES\"]))\n",
    "# ajustamos la codificacion\n",
    "column_trans.fit(data_to_pipe)\n",
    "xo_test = column_trans.transform(xo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42)\n",
    "x_train, x_val = column_trans.transform(x_train), column_trans.transform(x_val)\n",
    "oversample = SMOTE(sampling_strategy=0.7, random_state=1)\n",
    "x_train, y_train = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.85      0.86     19016\n",
      "           1       0.29      0.34      0.31      3492\n",
      "\n",
      "    accuracy                           0.77     22508\n",
      "   macro avg       0.58      0.59      0.58     22508\n",
      "weighted avg       0.78      0.77      0.77     22508\n",
      "\n"
     ]
    }
   ],
   "source": [
    "modelxgb = xgb.XGBClassifier(random_state=1, objective= 'binary:logistic',   eval_metric='aucpr', **previous_best)\n",
    "modelxgb = modelxgb.fit(x_train, y_train, eval_set=[(x_val, y_val)], verbose=False)\n",
    "predicted_xgb = modelxgb.predict(x_val)\n",
    "print(classification_report(predicted_xgb, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_xgb = modelxgb.predict(xo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dictionary[\"xgb_important_features\"] = average_precision_score(yo_test, predicted_xgb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Selección Mejor Modelo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observando los resultados encontrados bajo todas las pruebas previamente hechas, se puede notar que el mejor modelo encontrado fue el que utiliza SMOTE como método de oversampling y luego fue optimizado por grid search. A continuación mostramos los resultados y el valor de cross_validation que presenta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtenemos los datos del dataframe:\n",
    "data_to_pipe = data[COLUMNAS_BASALES]\n",
    "\n",
    "# volvemos a generar los datos originales con la nueva codificacion:\n",
    "Xo, Yo = data_to_pipe, data[\"atraso_15\"]\n",
    "_, xo_test, _, yo_test = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42, stratify=Yo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generamos una codificaciones ordinaria para los datos: \n",
    "column_trans = make_column_transformer((OrdinalEncoder(), COLUMNAS_BASALES))\n",
    "# ajustamos la codificacion\n",
    "column_trans.fit(data_to_pipe)\n",
    "xo_test = column_trans.transform(xo_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(Xo, Yo, test_size = 0.33, random_state = 42)\n",
    "x_train, x_val = column_trans.transform(x_train), column_trans.transform(x_val)\n",
    "oversample = SMOTE(sampling_strategy=0.7, random_state=42, k_neighbors=10)\n",
    "x_train, y_train = oversample.fit_resample(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'modelxgb_base': 0.20839491452833558,\n",
       " 'xgb_grid_base': 0.21728632479901608,\n",
       " 'xgb_grid_base_round2': 0.2185626724133346,\n",
       " 'xgb_smote': 0.24054706513149243,\n",
       " 'xgb_smote_under': 0.2284234527919285,\n",
       " 'xgb_grid_smote': 0.2415843165271429,\n",
       " 'more_features': 0.2376063877394071,\n",
       " 'xgb_important_features': 0.23492118067480844}"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUCPR result: 0.3116357724170816 +- 0.010272190469080204\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "kfolds = RepeatedStratifiedKFold(n_splits=5, random_state=32)\n",
    "kfolds.get_n_splits(Xo, Yo)\n",
    "X, Y = column_trans.transform(Xo), Yo\n",
    "result = cross_val_score(modelxgb, xo_test, yo_test, cv=kfolds, n_jobs=-1, scoring='average_precision')\n",
    "print(f\"AUCPR result: {result.mean()} +- {result.std()}\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "column_trans = make_column_transformer((OrdinalEncoder(), COLUMNAS_BASALES))\n",
    "column_trans.fit(data_to_pipe)\n",
    "pipe = make_pipeline(column_trans, xgb_smote_grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['backend/production_model.sav']"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "model_name = \"backend/production_model.sav\"\n",
    "joblib.dump(pipe, model_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A través de esta pregunta se pudo mostrar que fue posible mejorar el modelo generado más del 20%, dentro de las posibles mejoras que aún podrían hacerse se encuentra la posibilidad de agregar las distancias entre ciudad de origen y ciudad de destino, para que así el modelo tenga una referencia de que tan lejos quedan las ciudades y por ende pueda considerar esto en su predicción."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. API:\n",
    "# Pregunta 3: \n",
    "Habilita el modelo seleccionado como API REST para ser expuesto\n",
    "\n",
    "Para resolver este problema lo que se hizo en primera instancia fue generar un proyecto de Django. En dicho proyecto se generaró una api que contiene los métodos post y get, en específico el método post es el que se encarga de generar las predicciones para el equipo y es el que se encuentra a disposición para realizar las consultas. El método post debe realizarse con la siguiente url http://localhost:8000/api/estaAtrasado \n",
    "\n",
    "Dicha uri debe ser utilizada con un body que debe contener los siguiente: \n",
    "- OPERA\n",
    "- TIPOVUELO\n",
    "- MES\n",
    "- VLO_I\n",
    "\n",
    "los primeros tres elementos de la lista corresponden a los features utilizados por el modelo, mientras que VLO_I se utiliza como indicador del vuelo que se está buscando predecir si llegará con retraso. De esta manera, el equipo debe mandar el post request con los valores que corresponden a la base de datos, ya que el modelo de pipeline realizado es el que posteriormente se encarga de transformar los valores. Otro punto importante se encuentra en que la api generada puede trabajar con múltiples vuelos al mismo tiempo, esto se debe a que el api está hecha para poder mandar un arreglo de json (donde cada uno de ellos representa un vuelo), por lo que el payload final del método rest es el siguiente: \n",
    "\n",
    "{\"vuelos\": [{\"MES\": 3,\"TIPOVUELO\": \"N\", \"OPERA\": \"K.L.M.\", \"VLO_I\": 226}, {\"MES\": 12,\"TIPOVUELO\": \"I\", \"OPERA\": \"Air Canada\", \"VLO_I\":226}]}\n",
    "\n",
    "\n",
    "Además se realizó un método GET, dicho método trae el historial de consultas que se le han realizado al servidor, este utiliza la misma api que el método anterior. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pregunta 4:\n",
    "Haz pruebas de estrés a la API con el modelo expuesto con al menos 50.000 requests durante 45 segundos.\n",
    "Para esto debes utilizar esta herramienta y presentar las métricas obtenidas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cargamos en primera instancia la imagen de wrk desde docker: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker pull williamyeh/wrk"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hacemos un build del servidor desde docker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building web\n",
      "\u001b[1A\u001b[1B\u001b[0G\u001b[?25l[+] Building 0.0s (0/1)                                                         \n",
      "\u001b[?25h\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.1s (2/2)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.4s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.6s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              0.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.7s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 0.9s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              0.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.0s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              0.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              1.0s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.3s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              1.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              1.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.6s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              1.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.8s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              1.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 1.9s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              1.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.1s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              1.9s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.2s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              2.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.4s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              2.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.5s (2/3)                                                         \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.7s (4/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.1s\n",
      " => => transferring context: 30B                                           0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 2.8s (4/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.2s\n",
      " => => transferring context: 139.63kB                                      0.2s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.0s (4/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.3s\n",
      " => => transferring context: 140.60kB                                      0.3s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.1s (4/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.5s\n",
      " => => transferring context: 141.04kB                                      0.5s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.2s (4/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.6s\n",
      " => => transferring context: 141.56kB                                      0.6s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.3s (4/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.7s\n",
      " => => transferring context: 142.15kB                                      0.7s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.4s (4/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m => [internal] load build context                                          0.8s\n",
      " => => transferring context: 142.60kB                                      0.8s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.5s (8/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 142.81kB                                      0.8s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /backend                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY requirements.txt requirements.txt                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN pip3 install --upgrade pip && pip3 install -r requir  0.0s\n",
      "\u001b[0m => [5/6] COPY . .                                                         0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.7s (9/10)                                                        \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 142.81kB                                      0.8s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /backend                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY requirements.txt requirements.txt                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN pip3 install --upgrade pip && pip3 install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => [5/6] COPY . .                                                         0.1s\n",
      "\u001b[0m => [6/6] WORKDIR /backend                                                 0.1s\n",
      "\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.8s (10/11)                                                       \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 142.81kB                                      0.8s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /backend                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY requirements.txt requirements.txt                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN pip3 install --upgrade pip && pip3 install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => [5/6] COPY . .                                                         0.1s\n",
      "\u001b[0m\u001b[34m => [6/6] WORKDIR /backend                                                 0.1s\n",
      "\u001b[0m => exporting to image                                                     0.1s\n",
      "\u001b[34m => => exporting layers                                                    0.1s\n",
      "\u001b[0m\u001b[?25h\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[1A\u001b[0G\u001b[?25l[+] Building 3.9s (11/11) FINISHED                                              \n",
      "\u001b[34m => [internal] load build definition from Dockerfile                       0.1s\n",
      "\u001b[0m\u001b[34m => => transferring dockerfile: 32B                                        0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load .dockerignore                                          0.1s\n",
      "\u001b[0m\u001b[34m => => transferring context: 2B                                            0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load metadata for docker.io/library/python:3.8              2.4s\n",
      "\u001b[0m\u001b[34m => [1/6] FROM docker.io/library/python:3.8@sha256:90a35121aa2d413711ed40  0.0s\n",
      "\u001b[0m\u001b[34m => [internal] load build context                                          0.9s\n",
      "\u001b[0m\u001b[34m => => transferring context: 142.81kB                                      0.8s\n",
      "\u001b[0m\u001b[34m => CACHED [2/6] WORKDIR /backend                                          0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [3/6] COPY requirements.txt requirements.txt                    0.0s\n",
      "\u001b[0m\u001b[34m => CACHED [4/6] RUN pip3 install --upgrade pip && pip3 install -r requir  0.0s\n",
      "\u001b[0m\u001b[34m => [5/6] COPY . .                                                         0.1s\n",
      "\u001b[0m\u001b[34m => [6/6] WORKDIR /backend                                                 0.1s\n",
      "\u001b[0m\u001b[34m => exporting to image                                                     0.2s\n",
      "\u001b[0m\u001b[34m => => exporting layers                                                    0.1s\n",
      "\u001b[0m\u001b[34m => => writing image sha256:c6aecc2426df9f032b4cd2895746617f2a92bd9601a4a  0.0s\n",
      "\u001b[0m\u001b[34m => => naming to docker.io/library/neuralworkschallenge_web                0.0s\n",
      "\u001b[0m\u001b[?25h\n",
      "Use 'docker scan' to run Snyk tests against images to find vulnerabilities and learn how to fix them\n"
     ]
    }
   ],
   "source": [
    "! docker-compose build"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Levantamos el servidor en docker. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating network \"neuralworkschallenge_default\" with the default driver\n",
      "Creating neuralworkschallenge_web_1 ... \n",
      "\u001b[1Bting neuralworkschallenge_web_1 ... \u001b[32mdone\u001b[0m"
     ]
    }
   ],
   "source": [
    "! docker-compose up -d"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 POST Method: \n",
    "\n",
    "A modo de muestras se agrega el script que se utilizará para realizar las pruebas de estrés. \n",
    "\n",
    "%% Inicio %%\n",
    "\n",
    "wrk.method = \"POST\"\n",
    "\n",
    "wrk.body = '{\"vuelos\": [{\"MES\": 3,\"TIPOVUELO\": \"N\", \"OPERA\": \"K.L.M.\", \"VLO_I\": 226}, {\"MES\": 12,\"TIPOVUELO\": \"I\", \"OPERA\": \"Air Canada\", \"VLO_I\":226}]}'\n",
    "\n",
    "wrk.headers[\"Content-Type\"] = \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 45s test @ http://host.docker.internal:8000/api/estaAtrasado\n",
      "  10 threads and 50000 connections\n",
      "  Thread Stats   Avg      Stdev     Max   +/- Stdev\n",
      "    Latency     0.00us    0.00us   0.00us    -nan%\n",
      "    Req/Sec     0.00      0.00     0.00      -nan%\n",
      "  0 requests in 1.83m, 0.00B read\n",
      "  Socket errors: connect 21637, read 0, write 305, timeout 0\n",
      "Requests/sec:      0.00\n",
      "Transfer/sec:       0.00B\n"
     ]
    }
   ],
   "source": [
    "! docker run --net=\"host\" --rm -v `pwd`/scripts:/scripts williamyeh/wrk -t10 -c50000 -d45s -s /scripts/post_test.lua http://host.docker.internal:8000/api/estaAtrasado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11.1 Método Get: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 45s test @ http://host.docker.internal:8000/api/estaAtrasado\n",
      "  100 threads and 50000 connections\n",
      "  Thread Stats   Avg      Stdev     Max   +/- Stdev\n",
      "    Latency     0.00us    0.00us   0.00us    -nan%\n",
      "    Req/Sec     0.00      0.00     0.00      -nan%\n",
      "  0 requests in 1.01m, 1.43KB read\n",
      "  Socket errors: connect 20906, read 0, write 817, timeout 0\n",
      "Requests/sec:      0.00\n",
      "Transfer/sec:      24.09B\n"
     ]
    }
   ],
   "source": [
    "! docker run --net=\"host\" --rm -v scripts:/scripts williamyeh/wrk -t10000 -c50000 -d45s http://host.docker.internal:8000/api/estaAtrasado"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Propuesta de Mejora"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los resultados anteriores muestran que la api desarrollada tiene problemas al momento de trabajar con las 50000 conexiones, en específico se cree que el problema más que en la api en específico se debe a la configuración del servidor y a los recursos disponibles, ya que este se queda sin recursos al correr el test. De este modo como propuesta de mejora, se propone utilizar el caché en los request del servidor que sean muy frecuentes de manera de utilizar dichos resultados y no utilizar recursos con casos en que ya se tiene los resultados, esto sería especialmente útil en caso de un avión que ya haya sido ingresado y por ende no tenga necesidad de ingresarse nuevamente como consulta. \n",
    "\n",
    "Además, se cree que ante la gran cantidad de request que llegan según lo testeado anteriormente, se podría utilizar computación distributiva de manera de permitir que la carga total de las consultas no esté centrada en un solo computador, sino que en varios de manera de simplificar la carga. Además es posible utilizar aplicaciones cloud que están justamente planteadas como soluciones para los casos de obtención masiva de datos. \n",
    "\n",
    "Por último, otra posible mejora a la api propuesta es la utilización de apis asíncronas, las cuales permitan que las consultas sean recibidas por parte del servidor pero no sea necesario mantener una sesión abierta con el cliente. De esta manera se podría bajar la carga que generan conexiones abiertas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralWorks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d2ed49078c845dd85efea63a3cf8cac56d10a44a0e1e45ce190381046efb82b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
